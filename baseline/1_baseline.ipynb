{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install dependencies\n",
    "# !pip install datasets dashscope openai requests retrying numpy func_timeout bert_score transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# # load from local files in the repository\n",
    "# qas = load_dataset(\"../JJQA\",\"qa\")[\"train\"]\n",
    "# songs = load_dataset(\"../JJQA\",\"song\")[\"train\"]\n",
    "# song_index=json.loads(load_dataset(\"hobeter/JJQA\",\"song_index\")[\"train\"][\"dic\"][0])[0]\n",
    "\n",
    "# load from huggingface\n",
    "qas = load_dataset(\"hobeter/JJQA\",\"qa\")[\"train\"]\n",
    "songs = load_dataset(\"hobeter/JJQA\",\"song\")[\"train\"]\n",
    "song_index=json.loads(load_dataset(\"hobeter/JJQA\",\"song_index\")[\"train\"][\"dic\"][0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model&mode settings\n",
    "mode=\"with_rf\" #options: without_info with_whole_song with_rf; work when assistants_flag=False\n",
    "model=\"gpt-3.5-turbo\" #option: ernie-turbo chatglm2_6b_32k qwen-turbo baichuan2-7b-chat-v1 gpt-4 gpt-3.5-turbo gpt-4-1106-preview\n",
    "\n",
    "assistant_flag=False # whther to apply Assistants API for retrieval; only available in openai platform; if True, mode config would be ignored\n",
    "retrieval_file_path=\"../dataset/hf_song.json\"# file for retrieval when applying Assistants API\n",
    "\n",
    "\n",
    "# complete your keys here!!!\n",
    "qianfan_api_key=\"your_qianfan_api_key\"\n",
    "qianfan_secret_key=\"your_qianfan_secret_key\"\n",
    "\n",
    "dashscope_key=\"your_dashscope_key\"\n",
    "\n",
    "openai_key=\"your_openai_key\"\n",
    "\n",
    "#seconds for waiting when calling openai's apis\n",
    "wait_time=5\n",
    "\n",
    "# proxy settings\n",
    "proxy_flag=False \n",
    "proxy_url=\"http://127.0.0.1:7890\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual prompts\n",
    "\n",
    "system_prompt=\"你是一个基于林俊杰歌词的问答系统。\"\n",
    "\n",
    "prompt=\"不要考虑人称的变化，仅仅用简洁的短语回答以下相关问题，不要说明问题！不要直接重复整句歌词！\\n\"\n",
    "info_prompt=\"请根据以上的歌词信息，\"\n",
    "\n",
    "rf_prompt=\"已知一些相关歌词：\\n\"\n",
    "song_prompt=\"已知一首歌曲的歌词：\\n\"\n",
    "\n",
    "assistant_prompt=\"你是基于歌词的问答系统，请根据知识库中的相关信息回答问题。仅仅用简洁的短语回答，不要结合问题以完整的句子回答！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dashscope\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "from http import HTTPStatus\n",
    "import retrying\n",
    "from func_timeout import func_set_timeout\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "# for LLM calling\n",
    "class LLM():\n",
    "    def __init__(self) -> None:\n",
    "        self.model=model\n",
    "        self.platfrom=None\n",
    "        if(model in [\"ernie-turbo\",\"chatglm2_6b_32k\"]):\n",
    "            self.platform=\"qianfan\"\n",
    "            self.qianfan_api_key=qianfan_api_key\n",
    "            self.qianfan_secret_key=qianfan_secret_key\n",
    "            if(model==\"ernie-turbo\"):\n",
    "                self.url=\"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/eb-instant?access_token=\" + self.get_qianfan_access_token()\n",
    "            elif(model==\"chatglm2_6b_32k\"):\n",
    "                self.url=\"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/chatglm2_6b_32k?access_token=\" + self.get_qianfan_access_token()\n",
    "            else:\n",
    "                assert 1==0,f\"model {model} not supported\"\n",
    "        elif(model in [\"qwen-turbo\",\"baichuan2-7b-chat-v1\"]):\n",
    "            self.platform=\"dashscope\"\n",
    "            self.dashScope_key=dashscope_key\n",
    "            dashscope.api_key=self.dashScope_key\n",
    "        elif(model in [\"gpt-4\",\"gpt-3.5-turbo\",\"gpt-4-1106-preview\"]):\n",
    "            self.platform=\"openai\"\n",
    "            self.openai_key=openai_key\n",
    "\n",
    "            if(proxy_flag==True):\n",
    "                self.client = openai.OpenAI(\n",
    "                    api_key=self.openai_key,\n",
    "                    http_client=httpx.Client(\n",
    "                        proxies=proxy_url,\n",
    "                    ),\n",
    "                )\n",
    "            else:\n",
    "                self.client = openai.OpenAI(\n",
    "                    api_key=self.openai_key,\n",
    "                )\n",
    "\n",
    "            if(assistant_flag==True):\n",
    "                self.file = self.client.files.create(\n",
    "                    file=open(retrieval_file_path, \"rb\"),\n",
    "                    purpose='assistants'\n",
    "                )\n",
    "                self.assistant = self.client.beta.assistants.create(\n",
    "                    name=\"JJQA-bot\",\n",
    "                    instructions=assistant_prompt,\n",
    "                    tools=[{\"type\": \"retrieval\"}],\n",
    "                    model=self.model,\n",
    "                    file_ids=[self.file.id],\n",
    "                )\n",
    "                self.assistant_logs=\"\"\n",
    "\n",
    "        else:\n",
    "            assert 1==0,f\"model {model} not supported\"\n",
    "    \n",
    "    def get_qianfan_access_token(self):\n",
    "        url = \"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=\"+self.qianfan_api_key+\"&client_secret=\"+self.qianfan_secret_key\n",
    "        \n",
    "        payload = json.dumps(\"\")\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        \n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "        return response.json().get(\"access_token\")\n",
    "    \n",
    "    def assistant_postprocess(self,message):\n",
    "        assert len(message.content)==1,f\"message content length error\"\n",
    "        message_content=message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "\n",
    "        tmp=message_content.value\n",
    "        if(tmp[-1]==\".\"):\n",
    "            tmp=tmp[:-1]+\"。\"\n",
    "\n",
    "        res_s=tmp\n",
    "        log_s=tmp\n",
    "        \n",
    "        # Iterate over the annotations and add footnotes\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            # Replace the text with a footnote\n",
    "            res_s=res_s.replace(annotation.text,'')\n",
    "            log_s=log_s.replace(annotation.text, f' [{index}]')\n",
    "\n",
    "            # Gather citations based on annotation attributes\n",
    "            if (file_citation := getattr(annotation, 'file_citation', None)):\n",
    "                cited_file = self.client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
    "            elif (file_path := getattr(annotation, 'file_path', None)):\n",
    "                cited_file = self.client.files.retrieve(file_path.file_id)\n",
    "                citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
    "                # Note: File download functionality not implemented above for brevity\n",
    "\n",
    "        # Add footnotes to the end of the message before displaying to user\n",
    "        log_s += '\\n' + '\\n'.join(citations)\n",
    "\n",
    "        res_s=res_s.strip()\n",
    "        log_s=log_s.strip()\n",
    "        if(len(citations)!=0):\n",
    "            assert res_s!=log_s,f\"assistant postprocess error\"\n",
    "        return res_s,log_s,len(annotations)\n",
    "    \n",
    "    @retrying.retry(stop_max_attempt_number=5,wait_fixed=1*1000,)# retry when a error occurs\n",
    "    def call(self,input_t):\n",
    "        if(self.platform==\"qianfan\"):\n",
    "\n",
    "            payload=None\n",
    "            if(self.model in [\"ernie-turbo\"]):\n",
    "                payload = json.dumps({\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": input_t\n",
    "                        }\n",
    "                    ],\n",
    "                    # \"system\": system_prompt, #availavle for ernie\n",
    "                })\n",
    "            else:\n",
    "                payload = json.dumps({\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": input_t\n",
    "                        }\n",
    "                    ],\n",
    "                    # \"system\": system_prompt, #availavle for ernie\n",
    "                })\n",
    "\n",
    "            headers = {\n",
    "                'Content-Type': 'application/json'\n",
    "            }\n",
    "            response = requests.request(\"POST\", self.url, headers=headers, data=payload)\n",
    "            response_dic=json.loads(response.text)\n",
    "\n",
    "            if(\"error_code\" in list(response_dic.keys())):\n",
    "                assert 1==0,f\"error_code:{response_dic['error_code']},error_msg:{response_dic['error_msg']}\"\n",
    "\n",
    "            return response_dic[\"result\"].strip()\n",
    "        \n",
    "        elif(self.platform==\"dashscope\"):\n",
    "\n",
    "            messages = [\n",
    "                # {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': input_t},\n",
    "            ]\n",
    "            response = dashscope.Generation.call(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                result_format='message',  # set the result to be \"message\" format.\n",
    "            )\n",
    "            if response.status_code != HTTPStatus.OK:\n",
    "                if(\"inappropriate\" in response.message):  # calling fails due to the safety system of dashscope\n",
    "                    return \"inappropriate error\"\n",
    "                else:\n",
    "                    assert 1==0,'Request id: %s, Status code: %s, error code: %s, error message: %s' % (response.request_id, response.status_code,response.code, response.message)\n",
    "\n",
    "            return response.output.choices[0]['message']['content'].strip()\n",
    "        \n",
    "        elif(self.platform==\"openai\"):\n",
    "\n",
    "            @func_set_timeout(wait_time)\n",
    "            def openai_chat(model,input_t):\n",
    "                return self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        # {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": input_t}\n",
    "                    ],\n",
    "                )\n",
    "            \n",
    "            @func_set_timeout(wait_time*12*5)\n",
    "            def openai_assistant_check(thread_id,run_id):\n",
    "                while(1):\n",
    "                    run = self.client.beta.threads.runs.retrieve(\n",
    "                        thread_id=thread_id,\n",
    "                        run_id=run_id,\n",
    "                    )\n",
    "                    if(run.status==\"completed\"):\n",
    "                        return\n",
    "                    else:\n",
    "                        # print(run.status)\n",
    "                        time.sleep(1.0)\n",
    "            \n",
    "            if(assistant_flag==False):\n",
    "                try:\n",
    "                    response = openai_chat(self.model,input_t)\n",
    "                    return response.choices[0].message.content.strip()\n",
    "                except:\n",
    "                    print(\"timeout\")\n",
    "                    assert 1==0,f\"timeout\"\n",
    "            \n",
    "            else:\n",
    "                thread = self.client.beta.threads.create()\n",
    "                message = self.client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    role=\"user\",\n",
    "                    content=input_t,\n",
    "                )\n",
    "\n",
    "                run = self.client.beta.threads.runs.create(\n",
    "                    thread_id=thread.id,\n",
    "                    assistant_id=self.assistant.id,\n",
    "                    instructions=prompt,\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    openai_assistant_check(thread.id,run.id)\n",
    "                    messages = self.client.beta.threads.messages.list(thread_id=thread.id)\n",
    "                    self.client.beta.threads.delete(thread.id)\n",
    "                    assert len(messages.data)==2,f\"message.data length error\"\n",
    "                    message=messages.data[0]\n",
    "                    res_s,log_s,annos_l=self.assistant_postprocess(message)\n",
    "                    tmp_s=\"q:\\n\"+input_t+\"\\nannos:\"+str(annos_l)+\"\\nlogs:\\n\"+log_s+\"\\na:\\n\"+res_s+\"\\n\\n\"\n",
    "                    # print(tmp_s)\n",
    "                    self.assistant_logs=self.assistant_logs+tmp_s\n",
    "\n",
    "                    with open(f\"as_{model}_{retrieval_file_path.split('/')[-1].split('.')[0].strip()}_logs.txt\",\"w\") as f:\n",
    "                        f.write(self.assistant_logs)\n",
    "\n",
    "                    return res_s\n",
    "                except:\n",
    "                    print(\"error\")\n",
    "                    self.client.beta.threads.delete(thread.id)\n",
    "                    assert 1==0,f\"error\"\n",
    "\n",
    "    def free(self):\n",
    "        if(assistant_flag==True):\n",
    "            file_deletion_status = self.client.beta.assistants.files.delete(\n",
    "                assistant_id=self.assistant.id,\n",
    "                file_id=self.file.id\n",
    "            )\n",
    "            self.client.beta.assistants.delete(self.assistant.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "baseline_res={}\n",
    "\n",
    "llm=LLM()\n",
    "\n",
    "td=tqdm.tqdm(enumerate(qas),total=len(qas))\n",
    "\n",
    "#run the baseline\n",
    "for ind,qa in td:\n",
    "    q=qa[\"q\"]\n",
    "    a=qa[\"a\"]+\"。\"\n",
    "    rf=qa[\"rf\"].strip().split(\" \")\n",
    "    rf=[int(_) for _ in rf]\n",
    "    song=songs[song_index[qa[\"song_id\"]]]\n",
    "    lyric=song[\"lyric\"]\n",
    "    if(assistant_flag==True):\n",
    "        input_t=q+\"？\"\n",
    "        res=llm.call(input_t)\n",
    "    elif (mode==\"without_info\"):\n",
    "        input_t=prompt+q+\"？\"\n",
    "        res=llm.call(input_t)\n",
    "    elif(mode==\"with_rf\"):\n",
    "        input_t=rf_prompt\n",
    "        lyrics=lyric.strip().split(\"\\n\")\n",
    "        for line in rf:\n",
    "            input_t+=lyrics[line]+\"\\n\"\n",
    "        input_t=input_t+\"\\n\\n\"+info_prompt+prompt+q+\"？\"\n",
    "        res=llm.call(input_t)\n",
    "    elif(mode==\"with_whole_song\"):\n",
    "        input_t=song_prompt+\"\\n\"+lyric+\"\\n\\n\"+info_prompt+prompt+q+\"？\"\n",
    "        res=llm.call(input_t)\n",
    "    else:\n",
    "        assert 1==0,f\"mode {mode} not supported\"\n",
    "        \n",
    "    baseline_res[qa[\"id\"]]={\n",
    "        \"q\":q+\"？\",\n",
    "        \"pred\":res.strip(),\n",
    "        \"label\":a\n",
    "    }\n",
    "\n",
    "llm.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_name=\"\"\n",
    "if(assistant_flag==False):\n",
    "    file_name=f\"{model}_{mode}_dic.json\"\n",
    "else:\n",
    "    file_name=f\"as_{model}_{retrieval_file_path.split('/')[-1].split('.')[0].strip()}_dic.json\"\n",
    "\n",
    "while(os.path.exists(file_name)):\n",
    "    file_name=\"new_\"+file_name\n",
    "    \n",
    "#save results\n",
    "with open(file_name,\"w\",encoding='utf-8') as f:\n",
    "    json.dump(baseline_res,f,indent=2,ensure_ascii=False)\n",
    "\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
